{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from wilds import get_dataset\n",
    "\n",
    "from models import ColorMNISTConvNetGAP, SPDTwoLayerFC\n",
    "from spd.run_spd import get_lr_schedule_fn, get_lr_with_warmup\n",
    "from spd.hooks import HookedRootModule\n",
    "from spd.log import logger\n",
    "from spd.models.base import SPDModel\n",
    "from spd.module_utils import (\n",
    "    get_nested_module_attr,\n",
    "    collect_nested_module_attrs,\n",
    ")\n",
    "from spd.types import Probability\n",
    "from spd.utils import set_seed\n",
    "from train_mnist import SpuriousMNIST\n",
    "from delta_attr_run_spd import make_cf\n",
    "from delta_attr_run_spd import SPDConfig\n",
    "from delta_attr_run_spd import calculate_attributions\n",
    "from delta_attr_run_spd import make_cf_digit_swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = SPDConfig(\n",
    "        seed            = 0,\n",
    "        out_dir         = \"spd_models_new\",\n",
    "        batch_size      = 64,\n",
    "        lr              = 3e-4,         \n",
    "        lr_schedule      = \"linear\",\n",
    "        steps            = 10000,     \n",
    "        lr_warmup_pct   = 0.05,\n",
    "\n",
    "        C               = 2,              \n",
    "        m_fc1           = 320,            \n",
    "        m_fc2           = 320,\n",
    "\n",
    "        teacher_ckpt    = \"checkpoints/best_model.pth\",\n",
    "\n",
    "        param_match_coeff           = 0.0,    \n",
    "        relative_param_match_coeff  = 2.0,    \n",
    "        distill_coeff               = 1.0,    \n",
    "\n",
    "        alpha_condition     = 1.0,            \n",
    "        cond_coeff          = 0.0,            \n",
    "\n",
    "        topk                = None,              \n",
    "        batch_topk          = False,\n",
    "        topk_recon_coeff    = 0.0,            \n",
    "\n",
    "        schatten_coeff      = 0.0,\n",
    "        schatten_pnorm      = 0.9,\n",
    "        unit_norm_matrices  = False,\n",
    "        relative_schatten_coeff = 0.0,\n",
    "        lp_sparsity_coeff   = 0.0,            \n",
    "        pnorm               = None,\n",
    "        lambda_r            = 1.00,\n",
    "        mu_r                = 1.00,\n",
    "        warmup_steps        = 1000,\n",
    "\n",
    "        print_freq      = 50,\n",
    "        save_freq       = 1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1918384/383388284.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mnist_model.load_state_dict(torch.load(config.teacher_ckpt))\n",
      "/tmp/ipykernel_1918384/383388284.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spd_fc.load_state_dict(torch.load('spd_models_routing_loss/waterbird_spd_best.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SPDTwoLayerFC(\n",
       "  (fc1): LinearComponent(\n",
       "    (hook_pre): HookPoint()\n",
       "    (hook_component_acts): HookPoint()\n",
       "    (hook_post): HookPoint()\n",
       "  )\n",
       "  (fc2): LinearComponent(\n",
       "    (hook_pre): HookPoint()\n",
       "    (hook_component_acts): HookPoint()\n",
       "    (hook_post): HookPoint()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model = ColorMNISTConvNetGAP()\n",
    "mnist_model.load_state_dict(torch.load(config.teacher_ckpt))\n",
    "mnist_model.eval()\n",
    "mnist_model.to(device)\n",
    "\n",
    "spd_fc = SPDTwoLayerFC(\n",
    "    in_features=28*28,\n",
    "    hidden_dim=128,\n",
    "    num_classes=10,\n",
    "    C=config.C,\n",
    "    m_fc1=config.m_fc1,\n",
    "    m_fc2=config.m_fc2\n",
    ")\n",
    "\n",
    "spd_fc.load_state_dict(torch.load('spd_models_routing_loss/waterbird_spd_best.pth'))\n",
    "spd_fc.eval()\n",
    "spd_fc.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_subset = SpuriousMNIST(root_dir=\"colorized-MNIST/training\", transform=train_transform)\n",
    "\n",
    "loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = mnist_model.conv  # up to conv part\n",
    "teacher_fc1 = mnist_model.fc1\n",
    "teacher_fc2 = mnist_model.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "data_iter = iter(loader)\n",
    "epoch = 0\n",
    "\n",
    "# If you want param matching:\n",
    "param_names = [\"fc1\",\"fc2\"]\n",
    "n_params = 0\n",
    "for param_name in param_names:\n",
    "    n_params += get_nested_module_attr(mnist_model, param_name + \".weight\").numel()\n",
    "\n",
    "imgs, digit_label, background_label = batch\n",
    "imgs = imgs.to(device)\n",
    "digit_label = digit_label.to(device)\n",
    "background_label = background_label.to(device)\n",
    "imgs_cf = make_cf(imgs)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats_cf = trunk(imgs_cf).mean(1).flatten(1) \n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = trunk(imgs)\n",
    "    feats = feats.mean(dim=1)\n",
    "    feats = feats.flatten(1)  # [B, 512]\n",
    "\n",
    "#=========================\n",
    "# (2) feats_with_grad\n",
    "#=========================\n",
    "feats_with_grad = feats.detach().clone().requires_grad_(True)\n",
    "\n",
    "#=========================\n",
    "# (3) teacher forward pass manually, storing in a teacher_cache\n",
    "#=========================\n",
    "feats_cf_grad = feats_cf.detach().clone().requires_grad_(True)\n",
    "t_cache_cf = {}\n",
    "\n",
    "t_cache_cf[\"fc1.hook_pre\"]  = feats_cf_grad\n",
    "h1_cf = teacher_fc1(feats_cf_grad)\n",
    "t_cache_cf[\"fc1.hook_post\"] = h1_cf\n",
    "h_relu_cf = torch.relu(h1_cf)\n",
    "t_cache_cf[\"fc2.hook_pre\"]  = h_relu_cf\n",
    "teacher_out_cf              = teacher_fc2(h_relu_cf)\n",
    "t_cache_cf[\"fc2.hook_post\"] = teacher_out_cf\n",
    "\n",
    "teacher_cache = {}\n",
    "\n",
    "teacher_cache[\"fc1.hook_pre\"] = feats_with_grad\n",
    "teacher_h_pre = teacher_fc1(feats_with_grad)\n",
    "teacher_cache[\"fc1.hook_post\"] = teacher_h_pre\n",
    "\n",
    "teacher_h = torch.relu(teacher_h_pre)\n",
    "teacher_cache[\"fc2.hook_pre\"] = teacher_h\n",
    "\n",
    "teacher_out = teacher_fc2(teacher_h)\n",
    "teacher_cache[\"fc2.hook_post\"] = teacher_out\n",
    "\n",
    "#=========================\n",
    "# (4) SPD forward pass with hooking\n",
    "#=========================\n",
    "\n",
    "spd_fc.reset_hooks()\n",
    "cache_cf, fwd_hooks_cf, _ = spd_fc.get_caching_hooks()\n",
    "with spd_fc.hooks(fwd_hooks_cf, [], reset_hooks_end=True):\n",
    "    h1_spd_cf = spd_fc.fc1(feats_cf)\n",
    "    h_spd_cf  = torch.relu(h1_spd_cf)\n",
    "    spd_out_cf = spd_fc.fc2(h_spd_cf)\n",
    "spd_fc.reset_hooks()\n",
    "cache_dict, fwd_hooks, _ = spd_fc.get_caching_hooks()\n",
    "with spd_fc.hooks(fwd_hooks, [], reset_hooks_end=True):\n",
    "    spd_h_pre = spd_fc.fc1(feats)\n",
    "    spd_h = torch.relu(spd_h_pre)\n",
    "    spd_out = spd_fc.fc2(spd_h)\n",
    "\n",
    "#=========================\n",
    "# (5) gather SPD activations\n",
    "#=========================\n",
    "pre_cf  = {k:v for k,v in cache_cf.items() if k.endswith(\"hook_pre\")}\n",
    "post_cf = {k:v for k,v in cache_cf.items() if k.endswith(\"hook_post\")}\n",
    "comp_cf = {k.removesuffix(\".hook_component_acts\"):v\n",
    "        for k,v in cache_cf.items() if k.endswith(\"hook_component_acts\")}\n",
    "pre_weight_acts = {}\n",
    "post_weight_acts = {}\n",
    "comp_acts = {}\n",
    "for k,v in cache_dict.items():\n",
    "    if k.endswith(\"hook_pre\"):\n",
    "        pre_weight_acts[k] = v\n",
    "    elif k.endswith(\"hook_post\"):\n",
    "        post_weight_acts[k] = v\n",
    "    elif k.endswith(\"hook_component_acts\"):\n",
    "        comp_acts[k.removesuffix(\".hook_component_acts\")] = v  # e.g. \"fc1\", \"fc2\"\n",
    "\n",
    "#=========================\n",
    "# (6) teacher pre/post from teacher_cache\n",
    "#=========================\n",
    "teacher_pre_acts = {k:v for k,v in teacher_cache.items() if k.endswith(\"hook_pre\")}\n",
    "teacher_post_acts= {k:v for k,v in teacher_cache.items() if k.endswith(\"hook_post\")}\n",
    "\n",
    "#=========================\n",
    "# (7) calculate attributions\n",
    "#=========================\n",
    "attrib_cf = calculate_attributions(\n",
    "    model          = spd_fc,\n",
    "    input_x        = feats_cf,\n",
    "    out            = spd_out_cf,\n",
    "    teacher_out    = teacher_out_cf if config.distil_from_target else spd_out_cf,\n",
    "    pre_acts       = {k:v for k,v in t_cache_cf.items() if k.endswith(\"hook_pre\")},\n",
    "    post_acts      = {k:v for k,v in t_cache_cf.items() if k.endswith(\"hook_post\")},\n",
    "    component_acts = comp_cf,\n",
    "    attribution_type = config.attribution_type,\n",
    ")\n",
    "\n",
    "attributions = calculate_attributions(\n",
    "    model=spd_fc,\n",
    "    input_x=feats,\n",
    "    out=spd_out,\n",
    "    teacher_out=teacher_out if getattr(config,\"distil_from_target\",True) else spd_out,\n",
    "    pre_acts=teacher_pre_acts,\n",
    "    post_acts=teacher_post_acts,\n",
    "    component_acts=comp_acts,\n",
    "    attribution_type=config.attribution_type\n",
    ")\n",
    "\n",
    "delta_attrib = attributions - attrib_cf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta attrib for component 0: 8.545426368713379\n",
      "delta attrib for component 1: 4.292652130126953\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "When ablating component 0, this should show that the f(x | theta - a_0) = f( x'| theta - a_0)\n",
    "then a_0(x) = a_0(x')\n",
    "so |a_0(x) - a_0(x')| should be close to 0? \n",
    "'''\n",
    "print(f'delta attrib for component 0: {delta_attrib[:, 0].abs().mean()}')\n",
    "print(f'delta attrib for component 1: {delta_attrib[:, 1].abs().mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|f(x | theta) - f(x | theta - P) - f(x_cf | theta) + f(x_cf | theta - P)|: 15.57197093963623\n",
      "|f(x_cf | theta - P_0) - f(x | theta - P_0)|: 2.7106430530548096\n",
      "|f(x | theta) - f(x | theta - P) - f(x_d_cf | theta) + f(x_d_cf | theta - P)|: 13.398056030273438\n",
      "|f(x_cf | theta - P_0) - f(x | theta - P_0): 2.1955978870391846\n",
      "|f(x | theta) - f(x | theta - P_0): 8.833394050598145\n",
      "|f(x | theta) - f(x | theta - P_0): 5.780861854553223\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "imgs, digit_label, background_label = batch\n",
    "imgs = imgs.to(device)\n",
    "digit_label = digit_label.to(device)\n",
    "background_label = background_label.to(device)\n",
    "\n",
    "imgs_cf = make_cf(imgs)\n",
    "imgs_d_cf = make_cf_digit_swap(imgs, digit_label, background_label)\n",
    "\n",
    "feats = trunk(imgs).mean(dim=1).flatten(1)\n",
    "feats_cf = trunk(imgs_cf).mean(dim=1).flatten(1)\n",
    "feats_d_cf = trunk(imgs_d_cf).mean(dim=1).flatten(1)\n",
    "\n",
    "full_spd_weight_1 = spd_fc.fc1.A[0] @ spd_fc.fc1.B[0] +  spd_fc.fc1.A[1] @ spd_fc.fc1.B[1]\n",
    "full_spd_weight_2 = spd_fc.fc2.A[0] @ spd_fc.fc2.B[0] +  spd_fc.fc2.A[1] @ spd_fc.fc2.B[1]\n",
    "\n",
    "full_recon = feats @ full_spd_weight_1\n",
    "full_recon = torch.relu(full_recon)\n",
    "full_recon = full_recon @ full_spd_weight_2\n",
    "\n",
    "full_recon_cf = feats_cf @ full_spd_weight_1\n",
    "full_recon_cf = torch.relu(full_recon_cf)\n",
    "full_recon_cf = full_recon_cf @ full_spd_weight_2\n",
    "\n",
    "\n",
    "full_recon_d_cf = feats_d_cf @ full_spd_weight_1\n",
    "full_recon_d_cf = torch.relu(full_recon_d_cf)\n",
    "full_recon_d_cf = full_recon_d_cf @ full_spd_weight_2\n",
    "\n",
    "\n",
    "out_recon = spd_fc(feats) # using this because the original paper code does this for some reason, and not use teacher model\n",
    "out_recon_cf = spd_fc(feats_cf)\n",
    "out_recon_d_cf = spd_fc(feats_d_cf)\n",
    "\n",
    "spd_weight_1 = spd_fc.fc1.A[1] @ spd_fc.fc1.B[1]\n",
    "spd_weight_2 = spd_fc.fc2.A[1] @ spd_fc.fc2.B[1]\n",
    "\n",
    "out_spd_ablate = feats @ spd_weight_1 \n",
    "out_spd_ablate = torch.relu(out_spd_ablate)\n",
    "out_spd_ablate = out_spd_ablate @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_cf = feats_cf @ spd_weight_1 \n",
    "out_spd_ablate_cf = torch.relu(out_spd_ablate_cf)\n",
    "out_spd_ablate_cf = out_spd_ablate_cf @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_d_cf = feats_d_cf @ spd_weight_1 \n",
    "out_spd_ablate_d_cf = torch.relu(out_spd_ablate_d_cf)\n",
    "out_spd_ablate_d_cf = out_spd_ablate_d_cf @ spd_weight_2 \n",
    "\n",
    "true_attrib_scores = ((out_recon - out_spd_ablate) ** 2).mean(dim=-1) # f(x | theta) - f(x | theta - P)\n",
    "true_attrib_scores_cf = ((out_recon_cf - out_spd_ablate_cf)**2).mean(dim=-1) # f(x_cf | theta) - f(x_cf | theta - P)\n",
    "true_attrib_scores_d_cf = ((out_recon_d_cf - out_spd_ablate_d_cf)**2).mean(dim=-1) # f(x_d_cf | theta) - f(x_d_cf | theta - P)\n",
    "\n",
    "ablate_score_cf = out_spd_ablate - out_spd_ablate_cf # f(x | theta - P) - f(x_cf | theta - P), want this to be low for component 0 \n",
    "ablate_score_d_cf = out_spd_ablate - out_spd_ablate_d_cf # f(x | theta - P) - f(x_d_cf | theta - P), want this to be high for component 0 \n",
    "\n",
    "# true_attrib_scores - true_attrib_scores_cf = f(x | theta) - f(x | theta - P) - f(x_cf | theta) + f(x_cf | theta - P)\n",
    "# = f(x | theta) - f(x_cf | theta) + f(x_cf | theta - P) - f(x | theta - P) \n",
    "# = f(x | theta) - f(x_cf | theta) - ablate_score_cf \n",
    "\n",
    "print(f'|f(x | theta) - f(x | theta - P) - f(x_cf | theta) + f(x_cf | theta - P)|: {(true_attrib_scores - true_attrib_scores_cf).abs().mean()}')\n",
    "print(f'|f(x_cf | theta - P_0) - f(x | theta - P_0)|: {ablate_score_cf.abs().mean()}')\n",
    "\n",
    "print(f'|f(x | theta) - f(x | theta - P) - f(x_d_cf | theta) + f(x_d_cf | theta - P)|: {(true_attrib_scores - true_attrib_scores_d_cf).abs().mean()}')\n",
    "print(f'|f(x_cf | theta - P_0) - f(x | theta - P_0): {ablate_score_d_cf.abs().mean()}')\n",
    "\n",
    "print(f'|f(x | theta) - f(x | theta - P_0): {(full_recon - full_recon_cf).abs().mean()}')\n",
    "print(f'|f(x | theta) - f(x | theta - P_0): {(full_recon - full_recon_d_cf).abs().mean()}')\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Component 0 ablation score on background values (should be low I think): {((fin_ablate - fin_ablate_cf).abs()).mean()}')\n",
    "# print(f'Component 0 ablation score on background values (should be high I think): {(fin_ablate - fin_ablate_d_cf).abs().mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0 ablation score on background values (should be low I think): 10.33073616027832\n",
      "Component 0 ablation score on background values (should be high I think): 8.602184295654297\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "imgs, digit_label, background_label = batch\n",
    "imgs = imgs.to(device)\n",
    "digit_label = digit_label.to(device)\n",
    "background_label = background_label.to(device)\n",
    "\n",
    "imgs_cf = make_cf(imgs)\n",
    "imgs_d_cf = make_cf_digit_swap(imgs, digit_label, background_label)\n",
    "\n",
    "feats = trunk(imgs).mean(dim=1).flatten(1)\n",
    "feats_cf = trunk(imgs_cf).mean(dim=1).flatten(1)\n",
    "feats_d_cf = trunk(imgs_d_cf).mean(dim=1).flatten(1)\n",
    "\n",
    "out_recon = spd_fc(feats) # using this because the original paper code does this for some reason, and not use teacher model\n",
    "out_recon_cf = spd_fc(feats_cf)\n",
    "out_recon_d_cf = spd_fc(feats_d_cf)\n",
    "\n",
    "spd_weight_1 = spd_fc.fc1.A[1] @ spd_fc.fc1.B[1]\n",
    "spd_weight_2 = spd_fc.fc2.A[1] @ spd_fc.fc2.B[1]\n",
    "\n",
    "out_spd_ablate = feats @ spd_weight_1 \n",
    "out_spd_ablate = torch.relu(out_spd_ablate)\n",
    "out_spd_ablate = out_spd_ablate @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_cf = feats_cf @ spd_weight_1 \n",
    "out_spd_ablate_cf = torch.relu(out_spd_ablate_cf)\n",
    "out_spd_ablate_cf = out_spd_ablate_cf @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_d_cf = feats_d_cf @ spd_weight_1 \n",
    "out_spd_ablate_d_cf = torch.relu(out_spd_ablate_d_cf)\n",
    "out_spd_ablate_d_cf = out_spd_ablate_d_cf @ spd_weight_2 \n",
    "\n",
    "fin_ablate = ((out_recon - out_spd_ablate) ** 2).mean(dim=-1)\n",
    "fin_ablate_cf = out_spd_ablate - out_spd_ablate_cf\n",
    "fin_ablate_d_cf = (out_spd_ablate - out_spd_ablate_d_cf)\n",
    "\n",
    "print(f'Component 0 ablation score on background values (should be low I think): {((fin_ablate_cf)**2).mean()}')\n",
    "print(f'Component 0 ablation score on background values (should be high I think): {((fin_ablate_d_cf)**2).mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1 ablation score on background values (should be high I think): 4.569861888885498\n",
      "Component 1 ablation score on background values (should be low I think): 1.8238804340362549\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "imgs, digit_label, background_label = batch\n",
    "imgs = imgs.to(device)\n",
    "digit_label = digit_label.to(device)\n",
    "background_label = background_label.to(device)\n",
    "\n",
    "imgs_cf = make_cf(imgs)\n",
    "imgs_d_cf = make_cf_digit_swap(imgs, digit_label, background_label)\n",
    "\n",
    "feats = trunk(imgs).mean(dim=1).flatten(1)\n",
    "feats_cf = trunk(imgs_cf).mean(dim=1).flatten(1)\n",
    "feats_d_cf = trunk(imgs_d_cf).mean(dim=1).flatten(1)\n",
    "\n",
    "out_recon = spd_fc(feats) # using this because the original paper code does this for some reason, and not use teacher model\n",
    "out_recon_cf = spd_fc(feats_cf)\n",
    "out_recon_d_cf = spd_fc(feats_d_cf)\n",
    "\n",
    "spd_weight_1 = spd_fc.fc1.A[0] @ spd_fc.fc1.B[0]\n",
    "spd_weight_2 = spd_fc.fc2.A[0] @ spd_fc.fc2.B[0]\n",
    "\n",
    "out_spd_ablate = feats @ spd_weight_1 \n",
    "out_spd_ablate = torch.relu(out_spd_ablate)\n",
    "out_spd_ablate = out_spd_ablate @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_cf = feats_cf @ spd_weight_1 \n",
    "out_spd_ablate_cf = torch.relu(out_spd_ablate_cf)\n",
    "out_spd_ablate_cf = out_spd_ablate_cf @ spd_weight_2 \n",
    "\n",
    "out_spd_ablate_d_cf = feats_d_cf @ spd_weight_1 \n",
    "out_spd_ablate_d_cf = torch.relu(out_spd_ablate_d_cf)\n",
    "out_spd_ablate_d_cf = out_spd_ablate_d_cf @ spd_weight_2 \n",
    "\n",
    "fin_ablate = ((out_recon - out_spd_ablate) ** 2).mean(dim=-1)\n",
    "fin_ablate_cf = (out_spd_ablate - out_spd_ablate_cf)\n",
    "fin_ablate_d_cf = (out_spd_ablate - out_spd_ablate_d_cf)\n",
    "\n",
    "# out_recon = f(x | theta)\n",
    "# out_spd_ablate = f(x | theta - P)\n",
    "# f(x | theta - P ) - f(x' | theta  - P)\n",
    "# out_spd_ablate - out_spd_ablate_cf \n",
    "\n",
    "\n",
    "\n",
    "print(f'Component 1 ablation score on background values (should be high I think): {((fin_ablate_cf)**2).mean()}')\n",
    "print(f'Component 1 ablation score on background values (should be low I think): {((fin_ablate_d_cf)**2).mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
