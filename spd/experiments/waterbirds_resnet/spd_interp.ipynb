{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from wilds import get_dataset\n",
    "\n",
    "from models import WaterbirdResNet18, SPDTwoLayerFC\n",
    "from spd.run_spd import get_lr_schedule_fn, get_lr_with_warmup\n",
    "from spd.hooks import HookedRootModule\n",
    "from spd.log import logger\n",
    "from spd.models.base import SPDModel\n",
    "from spd.module_utils import (\n",
    "    get_nested_module_attr,\n",
    "    collect_nested_module_attrs,\n",
    ")\n",
    "from spd.types import Probability\n",
    "from spd.utils import set_seed\n",
    "from train_resnet import WaterbirdsSubset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 07:48:47 - INFO - Running SPD Waterbird with manual teacher caching, config=seed=27 batch_size=64 steps=966 lr=0.00017688154427024347 print_freq=50 save_freq=None out_dir=None distill_coeff=0.8673527736719934 param_match_coeff=2.6565345984830224 alpha_condition=1.0 cond_coeff=0.5058424630977391 C=40 m_fc1=16 m_fc2=16 lr_schedule='constant' lr_exponential_halflife=None lr_warmup_pct=0.0 unit_norm_matrices=False schatten_coeff=None schatten_pnorm=None teacher_ckpt='checkpoints/waterbird_resnet18_best.pth' topk=2.4825412692902824 batch_topk=True topk_recon_coeff=0.11381880979184406 distil_from_target=True lp_sparsity_coeff=0.009538420162649474 pnorm=2.0 attribution_type='gradient'\n",
      "/workspace/apd/spd/experiments/waterbirds_resnet/run_spd.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(config.teacher_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 11788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 07:48:48 - INFO - Step 0 | total=505.3057 distill=12.7612, param=490.7272, topk=1.4665, lp=0.0000, cond=0.3507, lr=1.77e-04\n",
      "2025-04-16 07:49:01 - INFO - Step 50 | total=323.1152 distill=0.0100, param=321.4576, topk=1.3039, lp=0.0076, cond=0.3360, lr=1.77e-04\n",
      "2025-04-16 07:49:15 - INFO - Step 100 | total=139.9305 distill=0.0018, param=139.3091, topk=0.3530, lp=0.0191, cond=0.2474, lr=1.77e-04\n",
      "2025-04-16 07:49:28 - INFO - Step 150 | total=40.2275 distill=0.0034, param=39.8569, topk=0.0847, lp=0.0161, cond=0.2663, lr=1.77e-04\n",
      "2025-04-16 07:49:41 - INFO - Step 200 | total=10.0520 distill=0.0010, param=9.7290, topk=0.0678, lp=0.0131, cond=0.2411, lr=1.77e-04\n",
      "2025-04-16 07:49:55 - INFO - Step 250 | total=2.9295 distill=0.0335, param=2.5366, topk=0.0741, lp=0.0116, cond=0.2737, lr=1.77e-04\n",
      "2025-04-16 07:50:08 - INFO - Step 300 | total=1.1580 distill=0.0410, param=0.7583, topk=0.0458, lp=0.0124, cond=0.3006, lr=1.77e-04\n",
      "2025-04-16 07:50:21 - INFO - Step 350 | total=0.6100 distill=0.0017, param=0.2556, topk=0.0702, lp=0.0082, cond=0.2742, lr=1.77e-04\n",
      "2025-04-16 07:50:34 - INFO - Step 400 | total=0.3761 distill=0.0011, param=0.0971, topk=0.0115, lp=0.0132, cond=0.2533, lr=1.77e-04\n",
      "2025-04-16 07:50:47 - INFO - Step 450 | total=0.3229 distill=0.0076, param=0.0379, topk=0.0080, lp=0.0144, cond=0.2551, lr=1.77e-04\n",
      "2025-04-16 07:51:01 - INFO - Step 500 | total=0.3252 distill=0.0218, param=0.0166, topk=0.0247, lp=0.0163, cond=0.2457, lr=1.77e-04\n",
      "2025-04-16 07:51:14 - INFO - Step 550 | total=0.2718 distill=0.0016, param=0.0071, topk=0.0042, lp=0.0124, cond=0.2464, lr=1.77e-04\n",
      "2025-04-16 07:51:27 - INFO - Step 600 | total=0.2754 distill=0.0003, param=0.0032, topk=0.0048, lp=0.0115, cond=0.2556, lr=1.77e-04\n",
      "2025-04-16 07:51:40 - INFO - Step 650 | total=0.2937 distill=0.0004, param=0.0017, topk=0.0057, lp=0.0073, cond=0.2785, lr=1.77e-04\n",
      "2025-04-16 07:51:54 - INFO - Step 700 | total=0.2808 distill=0.0075, param=0.0010, topk=0.0073, lp=0.0097, cond=0.2553, lr=1.77e-04\n",
      "2025-04-16 07:52:07 - INFO - Step 750 | total=0.2811 distill=0.0011, param=0.0006, topk=0.0084, lp=0.0144, cond=0.2566, lr=1.77e-04\n",
      "2025-04-16 07:52:20 - INFO - Step 800 | total=0.3443 distill=0.0355, param=0.0022, topk=0.0082, lp=0.0064, cond=0.2920, lr=1.77e-04\n",
      "2025-04-16 07:52:33 - INFO - Step 850 | total=0.2834 distill=0.0048, param=0.0014, topk=0.0057, lp=0.0121, cond=0.2594, lr=1.77e-04\n",
      "2025-04-16 07:52:47 - INFO - Step 900 | total=0.2827 distill=0.0005, param=0.0005, topk=0.0052, lp=0.0070, cond=0.2696, lr=1.77e-04\n",
      "2025-04-16 07:53:00 - INFO - Step 950 | total=0.2624 distill=0.0004, param=0.0003, topk=0.0091, lp=0.0118, cond=0.2407, lr=1.77e-04\n",
      "100%|█████████████████████████████████████████████████████████████| 967/967 [04:16<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss from rerun: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from run_spd import load_config_from_yaml, run_spd_waterbird\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Suppose you want to rerun trial #7\n",
    "config = load_config_from_yaml(\"optuna_trials/trial_27_config.yaml\")\n",
    "final_loss = run_spd_waterbird(config, device)\n",
    "print(\"Final Loss from rerun:\", final_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1840111/1031647141.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet_checkpoint = torch.load(resnet_ckpt_path, map_location=\"cpu\")\n",
      "/tmp/ipykernel_1840111/1031647141.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spd_state_dict = torch.load(spd_ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedWaterbirdModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (spd_model): SPDTwoLayerFC(\n",
       "    (fc1): LinearComponent(\n",
       "      (hook_pre): HookPoint()\n",
       "      (hook_component_acts): HookPoint()\n",
       "      (hook_post): HookPoint()\n",
       "    )\n",
       "    (fc2): LinearComponent(\n",
       "      (hook_pre): HookPoint()\n",
       "      (hook_component_acts): HookPoint()\n",
       "      (hook_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a combined model that uses the pretrained ResNet backbone \n",
    "# but replaces the FC layers with your SPD module\n",
    "class CombinedWaterbirdModel(nn.Module):\n",
    "    def __init__(self, resnet_model, spd_model):\n",
    "        super().__init__()\n",
    "        # Use only the feature extractor part of the ResNet18 model\n",
    "        self.features = resnet_model.features\n",
    "        # Use the SPD model for the fully connected part\n",
    "        self.spd_model = spd_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet backbone\n",
    "        feats = self.features(x)               # [B, 512, 1, 1]\n",
    "        feats = feats.flatten(start_dim=1)     # [B, 512]\n",
    "        \n",
    "        # Pass features to SPD module\n",
    "        out = self.spd_model(feats)\n",
    "        return out\n",
    "\n",
    "# 1. Load the pretrained ResNet model\n",
    "resnet_model = WaterbirdResNet18(num_classes=2, hidden_dim=512)\n",
    "resnet_ckpt_path = \"checkpoints/waterbird_resnet18_best.pth\"\n",
    "\n",
    "# The file showed the checkpoint has a different structure - it's a dictionary\n",
    "resnet_checkpoint = torch.load(resnet_ckpt_path, map_location=\"cpu\")\n",
    "resnet_model.load_state_dict(resnet_checkpoint['model_state_dict'])\n",
    "resnet_model.eval()\n",
    "\n",
    "# 2. Load the SPD model \n",
    "spd_model = SPDTwoLayerFC(\n",
    "    in_features=512,\n",
    "    hidden_dim=512,\n",
    "    num_classes=2,\n",
    "    C=40,\n",
    "    m_fc1=16,\n",
    "    m_fc2=16,\n",
    ")\n",
    "\n",
    "# Load SPD state dict\n",
    "spd_ckpt_path = \"optuna_trials/trial_29_spd.pth\"\n",
    "spd_state_dict = torch.load(spd_ckpt_path, map_location=\"cpu\")\n",
    "spd_model.load_state_dict(spd_state_dict)\n",
    "spd_model.eval()\n",
    "\n",
    "# 3. Create the combined model\n",
    "combined_model = CombinedWaterbirdModel(resnet_model, spd_model)\n",
    "combined_model.eval()\n",
    "\n",
    "# To use for inference:\n",
    "# with torch.no_grad():\n",
    "#     output = combined_model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 16, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_model.fc1.B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 11788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1246640/3955101530.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet_checkpoint = torch.load(resnet_ckpt_path, map_location=\"cpu\")\n",
      "/tmp/ipykernel_1246640/3955101530.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spd_state_dict = torch.load(spd_ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combined model on 1000 validation samples...\n",
      "Combined model accuracy on validation set: 71.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from wilds import get_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# First, setup validation dataset\n",
    "waterbird_dataset = get_dataset(dataset=\"waterbirds\", download=False)\n",
    "dataset_size = len(waterbird_dataset)\n",
    "print(f\"Total dataset size: {dataset_size}\")\n",
    "\n",
    "# Get indices\n",
    "all_indices = np.arange(dataset_size)\n",
    "np.random.shuffle(all_indices)\n",
    "train_indices = all_indices[:2000].tolist()\n",
    "val_indices = all_indices[2000:3000].tolist()  # Taking 1000 samples after the 2000th index\n",
    "\n",
    "# Setup validation transform\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Create validation subset\n",
    "val_subset = WaterbirdsSubset(\n",
    "    waterbird_dataset, \n",
    "    indices=val_indices,\n",
    "    transform=val_transform\n",
    ")\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the combined model\n",
    "# 1. Load ResNet\n",
    "resnet_model = WaterbirdResNet18(num_classes=2, hidden_dim=512)\n",
    "resnet_ckpt_path = \"checkpoints/waterbird_resnet18_best.pth\"\n",
    "resnet_checkpoint = torch.load(resnet_ckpt_path, map_location=\"cpu\")\n",
    "resnet_model.load_state_dict(resnet_checkpoint['model_state_dict'])\n",
    "resnet_model.eval()\n",
    "\n",
    "# 2. Load SPD model\n",
    "spd_model = SPDTwoLayerFC(\n",
    "    in_features=512,\n",
    "    hidden_dim=512,\n",
    "    num_classes=2,\n",
    "    C=40,\n",
    "    m_fc1=16,\n",
    "    m_fc2=16,\n",
    ")\n",
    "spd_ckpt_path = \"waterbird_spd_out/waterbird_spd_final.pth\"\n",
    "spd_state_dict = torch.load(spd_ckpt_path, map_location=\"cpu\")\n",
    "spd_model.load_state_dict(spd_state_dict)\n",
    "spd_model.eval()\n",
    "\n",
    "# 3. Create combined model\n",
    "class CombinedWaterbirdModel(nn.Module):\n",
    "    def __init__(self, resnet_model, spd_model):\n",
    "        super().__init__()\n",
    "        self.features = resnet_model.features\n",
    "        self.spd_model = spd_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.features(x)\n",
    "        feats = feats.flatten(start_dim=1)\n",
    "        out = self.spd_model(feats)\n",
    "        return out\n",
    "\n",
    "# Create the combined model and move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "combined_model = CombinedWaterbirdModel(resnet_model, spd_model).to(device)\n",
    "combined_model.eval()\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "metadata_correct = {}  # For analysis by metadata\n",
    "metadata_total = {}\n",
    "\n",
    "print(f\"Evaluating combined model on {len(val_indices)} validation samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, metadata in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = combined_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update overall stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Combined model accuracy on validation set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landbirds on land: 6220 samples\n",
      "Waterbirds on water: 1832 samples\n",
      "Landbirds on water: 2905 samples\n",
      "Waterbirds on land: 831 samples\n",
      "\n",
      "Evaluating original ResNet model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:26<00:00,  7.33it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.49it/s]\n",
      "100%|██████████| 91/91 [00:12<00:00,  7.14it/s]\n",
      "100%|██████████| 26/26 [00:03<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landbirds on land: 98.26%\n",
      "Waterbirds on water: 86.08%\n",
      "Landbirds on water: 25.44%\n",
      "Waterbirds on land: 4.93%\n",
      "\n",
      "Evaluating combined model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:26<00:00,  7.34it/s]\n",
      "100%|██████████| 58/58 [00:07<00:00,  7.41it/s]\n",
      "100%|██████████| 91/91 [00:12<00:00,  7.43it/s]\n",
      "100%|██████████| 26/26 [00:03<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landbirds on land: 98.31%\n",
      "Waterbirds on water: 85.81%\n",
      "Landbirds on water: 25.99%\n",
      "Waterbirds on land: 4.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from wilds import get_dataset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the dataset\n",
    "dataset = get_dataset(dataset=\"waterbirds\", download=False)\n",
    "\n",
    "# Create transform\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Define a function to get samples by group\n",
    "def get_group_indices(dataset, bird_type, background):\n",
    "    \"\"\"\n",
    "    Get indices of samples where:\n",
    "    - bird_type: 0 for landbird, 1 for waterbird\n",
    "    - background: 0 for land, 1 for water\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        x, y, metadata = dataset[i]\n",
    "        # y is the bird type, metadata[1] is the background type\n",
    "        if y == bird_type and metadata[0] == background:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "# Get indices for each group\n",
    "landbird_land = get_group_indices(dataset, 0, 0)  # Majority group\n",
    "waterbird_water = get_group_indices(dataset, 1, 1)  # Majority group\n",
    "landbird_water = get_group_indices(dataset, 0, 1)  # Minority group\n",
    "waterbird_land = get_group_indices(dataset, 1, 0)  # Minority group\n",
    "\n",
    "print(f\"Landbirds on land: {len(landbird_land)} samples\")\n",
    "print(f\"Waterbirds on water: {len(waterbird_water)} samples\")\n",
    "print(f\"Landbirds on water: {len(landbird_water)} samples\")  # This should be smaller\n",
    "print(f\"Waterbirds on land: {len(waterbird_land)} samples\")  # This should be smaller\n",
    "\n",
    "# Function to evaluate model on a specific group\n",
    "def evaluate_group(model, dataset, indices, transform, device, batch_size=32):\n",
    "    subset = Subset(dataset, indices)\n",
    "    \n",
    "    # Create a custom dataset that applies the transform\n",
    "    class TransformSubset:\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            x, y, metadata = self.subset[idx]\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            return x, y, metadata\n",
    "    \n",
    "    # Create the loader\n",
    "    loader = DataLoader(\n",
    "        TransformSubset(subset, transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, metadata in tqdm(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Now you can evaluate both your models on each group\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluate original ResNet model\n",
    "print(\"\\nEvaluating original ResNet model:\")\n",
    "resnet_model = resnet_model.to(device)\n",
    "landbird_land_acc = evaluate_group(resnet_model, dataset, landbird_land, transform, device)\n",
    "waterbird_water_acc = evaluate_group(resnet_model, dataset, waterbird_water, transform, device)\n",
    "landbird_water_acc = evaluate_group(resnet_model, dataset, landbird_water, transform, device)\n",
    "waterbird_land_acc = evaluate_group(resnet_model, dataset, waterbird_land, transform, device)\n",
    "\n",
    "print(f\"Landbirds on land: {landbird_land_acc:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc:.2f}%\")\n",
    "\n",
    "# Evaluate combined model\n",
    "print(\"\\nEvaluating combined model:\")\n",
    "combined_model = combined_model.to(device)\n",
    "landbird_land_acc = evaluate_group(combined_model, dataset, landbird_land, transform, device)\n",
    "waterbird_water_acc = evaluate_group(combined_model, dataset, waterbird_water, transform, device)\n",
    "landbird_water_acc = evaluate_group(combined_model, dataset, landbird_water, transform, device)\n",
    "waterbird_land_acc = evaluate_group(combined_model, dataset, waterbird_land, transform, device)\n",
    "\n",
    "print(f\"Landbirds on land: {landbird_land_acc:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combined model on 1000 validation samples, ablating circuit #0...\n",
      "Ablating circuit #0 => Accuracy on validation set: 71.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from wilds import get_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Same data-loading code as before ---\n",
    "# (omitted for brevity)\n",
    "\n",
    "# --- Same checkpoint loading for resnet_model and spd_model ---\n",
    "# (omitted for brevity)\n",
    "\n",
    "# Evaluate on validation set, but ablate circuit #0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(f\"Evaluating combined model on {len(val_indices)} validation samples, \"\n",
    "      \"ablating circuit #0...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_model.to(device).eval()\n",
    "spd_model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, metadata in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 1) Extract features from ResNet trunk\n",
    "        feats = resnet_model.features(inputs)\n",
    "        feats = feats.flatten(start_dim=1)  # shape [batch_size, 512]\n",
    "\n",
    "        # 2) Build the topk_mask that ablates circuit #0\n",
    "        batch_size = feats.size(0)\n",
    "        topk_mask = torch.ones((batch_size, spd_model.C), dtype=torch.bool, device=device)\n",
    "        topk_mask[:, 0] = False  # Turn off circuit #0 for every example\n",
    "\n",
    "        # 3) Forward pass through SPD with ablation\n",
    "        outputs = spd_model(feats, topk_mask=topk_mask)\n",
    "\n",
    "        # 4) Compute predictions\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Ablating circuit #0 => Accuracy on validation set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landbirds on land: 6220 samples\n",
      "Waterbirds on water: 1832 samples\n",
      "Landbirds on water: 2905 samples\n",
      "Waterbirds on land: 831 samples\n",
      "\n",
      "Evaluating combined model (no ablation):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 0%|          | 0/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 130\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Load models (assuming you've already loaded them before)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# resnet_model = ... (your ResNet model)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# spd_model = ... (your SPD model)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# First, evaluate the standard model performance (without ablation)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating combined model (no ablation):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m landbird_land_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspd_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandbird_land\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m waterbird_water_acc \u001b[38;5;241m=\u001b[39m evaluate_group(resnet_model, spd_model, dataset, waterbird_water, transform, device)\n\u001b[1;32m    132\u001b[0m landbird_water_acc \u001b[38;5;241m=\u001b[39m evaluate_group(resnet_model, spd_model, dataset, landbird_water, transform, device)\n",
      "Cell \u001b[0;32mIn[9], line 97\u001b[0m, in \u001b[0;36mevaluate_group\u001b[0;34m(resnet_model, spd_model, dataset, indices, transform, device, batch_size, ablate_circuits)\u001b[0m\n\u001b[1;32m     94\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract features using ResNet\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mresnet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m feats \u001b[38;5;241m=\u001b[39m feats\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 512]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Create ablation mask if needed\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from wilds import get_dataset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the dataset\n",
    "dataset = get_dataset(dataset=\"waterbirds\", download=False)\n",
    "\n",
    "# Create transform\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Define a function to get samples by group\n",
    "def get_group_indices(dataset, bird_type, background):\n",
    "    \"\"\"\n",
    "    Get indices of samples where:\n",
    "    - bird_type: 0 for landbird, 1 for waterbird\n",
    "    - background: 0 for land, 1 for water\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        x, y, metadata = dataset[i]\n",
    "        # y is the bird type, metadata[0] is the background type\n",
    "        if y == bird_type and metadata[0] == background:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "# Get indices for each group\n",
    "landbird_land = get_group_indices(dataset, 0, 0)  # Majority group\n",
    "waterbird_water = get_group_indices(dataset, 1, 1)  # Majority group\n",
    "landbird_water = get_group_indices(dataset, 0, 1)  # Minority group\n",
    "waterbird_land = get_group_indices(dataset, 1, 0)  # Minority group\n",
    "\n",
    "print(f\"Landbirds on land: {len(landbird_land)} samples\")\n",
    "print(f\"Waterbirds on water: {len(waterbird_water)} samples\")\n",
    "print(f\"Landbirds on water: {len(landbird_water)} samples\")\n",
    "print(f\"Waterbirds on land: {len(waterbird_land)} samples\")\n",
    "\n",
    "# Modified function to evaluate model on a specific group with optional circuit ablation\n",
    "def evaluate_group(resnet_model, spd_model, dataset, indices, transform, device, \n",
    "                  batch_size=32, ablate_circuits=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on a specific group with optional circuit ablation\n",
    "    \n",
    "    Args:\n",
    "        resnet_model: Feature extractor model\n",
    "        spd_model: SPD model with circuits\n",
    "        dataset: Dataset object\n",
    "        indices: Indices to evaluate on\n",
    "        transform: Image transform\n",
    "        device: Device to run on\n",
    "        batch_size: Batch size for evaluation\n",
    "        ablate_circuits: List of circuit indices to ablate (set to None for no ablation)\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Accuracy on the evaluated group\n",
    "    \"\"\"\n",
    "    subset = Subset(dataset, indices)\n",
    "    \n",
    "    # Create a custom dataset that applies the transform\n",
    "    class TransformSubset:\n",
    "        def __init__(self, subset, transform):\n",
    "            self.subset = subset\n",
    "            self.transform = transform\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            x, y, metadata = self.subset[idx]\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            return x, y, metadata\n",
    "    \n",
    "    # Create the loader\n",
    "    loader = DataLoader(\n",
    "        TransformSubset(subset, transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    resnet_model.eval()\n",
    "    spd_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, metadata in tqdm(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Extract features using ResNet\n",
    "            feats = resnet_model.features(inputs)\n",
    "            feats = feats.flatten(start_dim=1)  # [B, 512]\n",
    "            \n",
    "            # Create ablation mask if needed\n",
    "            if ablate_circuits is not None and len(ablate_circuits) > 0:\n",
    "                batch_size = feats.size(0)\n",
    "                topk_mask = torch.ones((batch_size, spd_model.C), dtype=torch.bool, device=device)\n",
    "                for circuit_idx in ablate_circuits:\n",
    "                    topk_mask[:, circuit_idx] = False  # Turn off specified circuits\n",
    "                \n",
    "                # Forward pass through SPD with ablation\n",
    "                outputs = spd_model(feats, topk_mask=topk_mask)\n",
    "            else:\n",
    "                # Normal forward pass through SPD without ablation\n",
    "                outputs = spd_model(feats)\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Now you can evaluate both models on each group\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models (assuming you've already loaded them before)\n",
    "# resnet_model = ... (your ResNet model)\n",
    "# spd_model = ... (your SPD model)\n",
    "\n",
    "# First, evaluate the standard model performance (without ablation)\n",
    "print(\"\\nEvaluating combined model (no ablation):\")\n",
    "landbird_land_acc = evaluate_group(resnet_model, spd_model, dataset, landbird_land, transform, device)\n",
    "waterbird_water_acc = evaluate_group(resnet_model, spd_model, dataset, waterbird_water, transform, device)\n",
    "landbird_water_acc = evaluate_group(resnet_model, spd_model, dataset, landbird_water, transform, device)\n",
    "waterbird_land_acc = evaluate_group(resnet_model, spd_model, dataset, waterbird_land, transform, device)\n",
    "\n",
    "print(f\"Landbirds on land: {landbird_land_acc:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc:.2f}%\")\n",
    "\n",
    "# Now evaluate with circuit #0 ablated\n",
    "ablate_circuits = [0]  # Ablate circuit #0\n",
    "print(f\"\\nEvaluating combined model with circuit(s) {ablate_circuits} ablated:\")\n",
    "landbird_land_acc_abl = evaluate_group(resnet_model, spd_model, dataset, landbird_land, transform, device, ablate_circuits=ablate_circuits)\n",
    "waterbird_water_acc_abl = evaluate_group(resnet_model, spd_model, dataset, waterbird_water, transform, device, ablate_circuits=ablate_circuits)\n",
    "landbird_water_acc_abl = evaluate_group(resnet_model, spd_model, dataset, landbird_water, transform, device, ablate_circuits=ablate_circuits)\n",
    "waterbird_land_acc_abl = evaluate_group(resnet_model, spd_model, dataset, waterbird_land, transform, device, ablate_circuits=ablate_circuits)\n",
    "\n",
    "print(f\"Landbirds on land: {landbird_land_acc_abl:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc_abl:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc_abl:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc_abl:.2f}%\")\n",
    "\n",
    "# Print the differences\n",
    "print(\"\\nAccuracy differences (ablated - normal):\")\n",
    "print(f\"Landbirds on land: {landbird_land_acc_abl - landbird_land_acc:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc_abl - waterbird_water_acc:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc_abl - landbird_water_acc:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc_abl - waterbird_land_acc:.2f}%\")\n",
    "\n",
    "# You can easily test ablating multiple circuits\n",
    "ablate_circuits = [0, 1, 2]  # Ablate circuits 0, 1, and 2\n",
    "print(f\"\\nEvaluating combined model with circuit(s) {ablate_circuits} ablated:\")\n",
    "landbird_land_acc_multi = evaluate_group(resnet_model, spd_model, dataset, landbird_land, transform, device, ablate_circuits=ablate_circuits)\n",
    "waterbird_water_acc_multi = evaluate_group(resnet_model, spd_model, dataset, waterbird_water, transform, device, ablate_circuits=ablate_circuits)\n",
    "landbird_water_acc_multi = evaluate_group(resnet_model, spd_model, dataset, landbird_water, transform, device, ablate_circuits=ablate_circuits)\n",
    "waterbird_land_acc_multi = evaluate_group(resnet_model, spd_model, dataset, waterbird_land, transform, device, ablate_circuits=ablate_circuits)\n",
    "\n",
    "print(f\"Landbirds on land: {landbird_land_acc_multi:.2f}%\")\n",
    "print(f\"Waterbirds on water: {waterbird_water_acc_multi:.2f}%\")\n",
    "print(f\"Landbirds on water: {landbird_water_acc_multi:.2f}%\")\n",
    "print(f\"Waterbirds on land: {waterbird_land_acc_multi:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests to confirm that this is working\n",
    "### 1) Plotting distribution of attribution scores on samples where water is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teacher_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mteacher_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'teacher_model' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SPDTwoLayerFC' from 'spd.run_spd' (/workspace/apd/spd/run_spd.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Imports from your own code\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# (Adjust paths/modules as necessary—e.g., from spd.run_spd import calculate_attributions, etc.)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaterbirdResNet18\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_spd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_attributions, SPDTwoLayerFC\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_resnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaterbirdsSubset  \u001b[38;5;66;03m# or your local subset wrapper\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SPDTwoLayerFC' from 'spd.run_spd' (/workspace/apd/spd/run_spd.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Imports from your own code\n",
    "# (Adjust paths/modules as necessary—e.g., from spd.run_spd import calculate_attributions, etc.)\n",
    "from models import WaterbirdResNet18\n",
    "from spd.run_spd import calculate_attributions, SPDTwoLayerFC\n",
    "from spd.utils import set_seed\n",
    "from train_resnet import WaterbirdsSubset  # or your local subset wrapper\n",
    "from wilds import get_dataset\n",
    "\n",
    "def gather_gradient_attributions_for_water(\n",
    "    spd_ckpt_path: str = \"waterbird_spd_final.pth\",\n",
    "    teacher_ckpt_path: str = \"checkpoints/waterbird_resnet18_best.pth\",\n",
    "    batch_size: int = 32,\n",
    "    seed: int = 0,\n",
    "    device: str = \"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads teacher (ResNet) and SPD final-layers, filters Waterbirds to only water background,\n",
    "    then replicates the EXACT gradient-based attribution steps from run_spd_waterbird.py.\n",
    "    Returns a list of [B, C] tensors, one per batch, where each entry is the subcomponent's\n",
    "    gradient-based attribution score for that batch.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load teacher (ResNet18) – must match how you trained SPD\n",
    "    teacher_model = WaterbirdResNet18(num_classes=2, hidden_dim=512)\n",
    "    teacher_ckpt = torch.load(teacher_ckpt_path, map_location=\"cpu\")\n",
    "    if \"model_state_dict\" in teacher_ckpt:\n",
    "        teacher_model.load_state_dict(teacher_ckpt[\"model_state_dict\"], strict=False)\n",
    "    else:\n",
    "        teacher_model.load_state_dict(teacher_ckpt, strict=False)\n",
    "    teacher_model.to(device)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Separate the trunk and final layers from teacher\n",
    "    teacher_trunk = teacher_model.features\n",
    "    teacher_fc1 = teacher_model.fc1\n",
    "    teacher_fc2 = teacher_model.fc2\n",
    "\n",
    "    # 2) Load SPD final-layers (same arch as your config)\n",
    "    spd_fc = SPDTwoLayerFC(\n",
    "        in_features=512,\n",
    "        hidden_dim=512,\n",
    "        num_classes=2,\n",
    "        C=40,      # must match your SPD config\n",
    "        m_fc1=16,  # likewise\n",
    "        m_fc2=16\n",
    "    )\n",
    "    spd_fc.load_state_dict(torch.load(spd_ckpt_path, map_location=\"cpu\"))\n",
    "    spd_fc.to(device)\n",
    "    spd_fc.eval()\n",
    "\n",
    "    # 3) Filter Waterbirds for background == \"water\".\n",
    "    #    Depending on your WILDS version, the metadata indexing might differ:\n",
    "    #       - Some store water/land at meta[2], others at meta[1].\n",
    "    #       - Double-check in your code and adjust if needed.\n",
    "    waterbird_ds = get_dataset(\"waterbirds\", download=False)\n",
    "    water_indices = []\n",
    "    for idx in range(len(waterbird_ds)):\n",
    "        _, _, meta = waterbird_ds[idx]\n",
    "        # For standard WILDS waterbirds, meta = [y, place, background].\n",
    "        # background==1 => water. If your code differs, adjust here.\n",
    "        if meta[2] == 1:  \n",
    "            water_indices.append(idx)\n",
    "\n",
    "    transform = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "\n",
    "    # Use your custom subset + transform\n",
    "    water_subset = WaterbirdsSubset(waterbird_ds, water_indices, transform=transform)\n",
    "    water_loader = DataLoader(water_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 4) Iterate over water images and replicate the run_spd logic for gradient attributions\n",
    "    all_attributions = []  # will store one [B, C] attribution tensor per batch\n",
    "\n",
    "    for (imgs, _, _) in water_loader:\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # 4a) Get trunk features\n",
    "        # Exactly like run_spd, we feed plain images -> trunk -> flatten\n",
    "        with torch.no_grad():\n",
    "            feats = teacher_trunk(imgs)\n",
    "            feats = feats.flatten(1)  # [B, 512]\n",
    "\n",
    "        # 4b) Teacher forward pass with feats_with_grad so we can do gradient-based attributions\n",
    "        feats_with_grad = feats.detach().clone().requires_grad_(True)\n",
    "\n",
    "        teacher_cache = {}\n",
    "        teacher_cache[\"fc1.hook_pre\"] = feats_with_grad\n",
    "        teacher_h_pre = teacher_fc1(feats_with_grad)\n",
    "        teacher_cache[\"fc1.hook_post\"] = teacher_h_pre\n",
    "\n",
    "        teacher_h = torch.relu(teacher_h_pre)\n",
    "        teacher_cache[\"fc2.hook_pre\"] = teacher_h\n",
    "\n",
    "        teacher_out = teacher_fc2(teacher_h)\n",
    "        teacher_cache[\"fc2.hook_post\"] = teacher_out\n",
    "\n",
    "        # 4c) SPD forward pass with hooking\n",
    "        spd_fc.reset_hooks()\n",
    "        cache_dict, fwd_hooks, _ = spd_fc.get_caching_hooks()\n",
    "\n",
    "        # no_grad is okay for SPD forward, since we only need the hooking of forward activations\n",
    "        with torch.no_grad(), spd_fc.hooks(fwd_hooks, [], reset_hooks_end=True):\n",
    "            spd_h_pre = spd_fc.fc1(feats)\n",
    "            spd_h = torch.relu(spd_h_pre)\n",
    "            spd_out = spd_fc.fc2(spd_h)\n",
    "\n",
    "        # 4d) Gather SPD pre/post/comp\n",
    "        pre_weight_acts = {}\n",
    "        post_weight_acts = {}\n",
    "        comp_acts = {}\n",
    "        for k,v in cache_dict.items():\n",
    "            if k.endswith(\"hook_pre\"):\n",
    "                pre_weight_acts[k] = v\n",
    "            elif k.endswith(\"hook_post\"):\n",
    "                post_weight_acts[k] = v\n",
    "            elif k.endswith(\"hook_component_acts\"):\n",
    "                # e.g. \"fc1.hook_component_acts\" => \"fc1\"\n",
    "                layer_name = k.removesuffix(\".hook_component_acts\")\n",
    "                comp_acts[layer_name] = v\n",
    "\n",
    "        teacher_pre_acts = {k:v for k,v in teacher_cache.items() if k.endswith(\"hook_pre\")}\n",
    "        teacher_post_acts= {k:v for k,v in teacher_cache.items() if k.endswith(\"hook_post\")}\n",
    "\n",
    "        # 4e) Calculate gradient-based attributions wrt TEACHER OUT\n",
    "        # Exactly as your run_spd does (the function does autograd.grad(teacher_out.sum(), ...)).\n",
    "        # Ensure `attribution_type='gradient'`.\n",
    "        # The teacher_out here is shape [B, 2], so .sum() is a scalar.\n",
    "        attributions = calculate_attributions(\n",
    "            model=spd_fc,\n",
    "            input_x=feats,            # SPD used feats without grad\n",
    "            out=spd_out,             # SPD's final output\n",
    "            teacher_out=teacher_out, # We'll do grad wrt teacher_out\n",
    "            pre_acts=teacher_pre_acts,\n",
    "            post_acts=teacher_post_acts,\n",
    "            component_acts=comp_acts,\n",
    "            attribution_type=\"gradient\"\n",
    "        )\n",
    "        # attributions => shape [B, C]\n",
    "\n",
    "        # 4f) Store for later\n",
    "        all_attributions.append(attributions.cpu())\n",
    "\n",
    "    # 5) all_attributions is now a list of Tensors, one per batch\n",
    "    #    You can cat them or keep them separate. For example, to combine them:\n",
    "    all_attributions = torch.cat(all_attributions, dim=0)  # shape [N, C], all water images\n",
    "    return all_attributions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    water_attr = gather_gradient_attributions_for_water(\n",
    "        spd_ckpt_path=\"waterbird_spd_final.pth\",\n",
    "        teacher_ckpt_path=\"checkpoints/waterbird_resnet18_best.pth\",\n",
    "        batch_size=32,\n",
    "        seed=0,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    print(\"Gradient-based SPD attributions for water backgrounds:\", water_attr.shape)\n",
    "    # For example, you might save them:\n",
    "    torch.save(water_attr, \"water_attr.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img, _, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mloader\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "img, _, meta = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
