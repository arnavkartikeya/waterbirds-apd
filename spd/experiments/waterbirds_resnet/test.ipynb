{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from wilds import get_dataset\n",
    "\n",
    "from models import WaterbirdResNet18, SPDTwoLayerFC\n",
    "from spd.hooks import HookedRootModule\n",
    "from spd.log import logger\n",
    "from spd.models.base import SPDModel\n",
    "from spd.module_utils import (\n",
    "    get_nested_module_attr,\n",
    "    collect_nested_module_attrs,\n",
    ")\n",
    "from spd.types import Probability\n",
    "from spd.utils import set_seed\n",
    "from train_resnet import WaterbirdsSubset\n",
    "from run_spd import WaterbirdSPDConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def calc_topk_mask(attribution_scores: torch.Tensor, topk: float, batch_topk: bool) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute a boolean mask for top-k subcomponents. Same logic as TMS:\n",
    "      - if batch_topk=True, multiply topk by batch_size & sort globally\n",
    "      - else take topk as an integer K\n",
    "    \"\"\"\n",
    "    batch_size = attribution_scores.shape[0]\n",
    "    # topk is float that might be fraction of batch_size\n",
    "    # we handle rounding, etc.\n",
    "    k = int(topk * batch_size) if batch_topk else int(topk)\n",
    "\n",
    "    if batch_topk:\n",
    "        # flatten \"b c\" -> \" (b c)\"\n",
    "        # find topk globally\n",
    "        shape_ = attribution_scores.shape\n",
    "        reshaped = einops.rearrange(attribution_scores, \"b ... c -> ... (b c)\")\n",
    "        topk_indices = torch.topk(reshaped, k, dim=-1).indices\n",
    "        mask = torch.zeros_like(reshaped, dtype=torch.bool)\n",
    "        mask.scatter_(dim=-1, index=topk_indices, value=True)\n",
    "        # reshape back\n",
    "        mask = einops.rearrange(mask, \"... (b c) -> b ... c\", b=batch_size)\n",
    "        return mask\n",
    "    else:\n",
    "        # topk in the last dimension for each sample\n",
    "        topk_indices = attribution_scores.topk(k, dim=-1).indices\n",
    "        mask = torch.zeros_like(attribution_scores, dtype=torch.bool)\n",
    "        mask.scatter_(dim=-1, index=topk_indices, value=True)\n",
    "        return mask\n",
    "\n",
    "\n",
    "def calc_lp_sparsity_loss(\n",
    "    out: torch.Tensor,\n",
    "    attributions: torch.Tensor,\n",
    "    step_pnorm: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Lp-sparsity on subnetwork attributions. Similar to TMS code:\n",
    "       - we first normalize by out.shape[-1]\n",
    "       - then raise absolute value to step_pnorm*0.5\n",
    "    \"\"\"\n",
    "    # e.g. divide attributions by out_dim\n",
    "    d_model_out = out.shape[-1]\n",
    "    scaled_attrib = attributions / d_model_out\n",
    "    # do (abs(...) + 1e-16)**(0.5*step_pnorm)\n",
    "    return (scaled_attrib.abs() + 1e-16) ** (0.5 * step_pnorm)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def calc_activation_attributions(\n",
    "    component_acts: dict[str, torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Example \"activation\" approach: sum of L2 over the subcomponent_acts\n",
    "    shape: (batch, C) or (batch, n_instances, C).\n",
    "    \"\"\"\n",
    "    # Just sum up squares of each subcomponent\n",
    "    # e.g. each entry in component_acts is (batch, C, d_out)\n",
    "    # sum over d_out dimension\n",
    "    first_key = next(iter(component_acts.keys()))\n",
    "    out_shape = component_acts[first_key].shape[:-1]  # strip d_out\n",
    "    attributions = torch.zeros(out_shape, device=component_acts[first_key].device)\n",
    "    for val in component_acts.values():\n",
    "        attributions += val.pow(2).sum(dim=-1)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def calc_grad_attributions(\n",
    "    model_out: torch.Tensor,  # teacher or spd output\n",
    "    post_weight_acts: dict[str, torch.Tensor],\n",
    "    pre_weight_acts: dict[str, torch.Tensor],\n",
    "    component_weights: dict[str, torch.Tensor],\n",
    "    C: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Like TMS's gradient approach: for each output dimension,\n",
    "    grad wrt post acts * subcomponent partial forward -> subcomponent attribution\n",
    "    Summed across output dims and squared.\n",
    "    \"\"\"\n",
    "    import torch.autograd as autograd\n",
    "\n",
    "    # unify keys\n",
    "    post_names = [k.removesuffix(\".hook_post\") for k in post_weight_acts.keys()]\n",
    "    pre_names = [k.removesuffix(\".hook_pre\") for k in pre_weight_acts.keys()]\n",
    "    comp_names = list(component_weights.keys())\n",
    "    assert set(post_names) == set(pre_names) == set(comp_names), \"layer name mismatch\"\n",
    "\n",
    "    batch_prefix = model_out.shape[:-1]  # e.g. (batch,) or (batch, n_inst)\n",
    "    out_dim = model_out.shape[-1]\n",
    "    attribution_scores = torch.zeros((*batch_prefix, C), device=model_out.device)\n",
    "\n",
    "    # get subcomponent partial forward\n",
    "    component_acts = {}\n",
    "    for nm in pre_names:\n",
    "        # shape pre: (batch..., d_in), comp_W: (C, d_in, d_out)\n",
    "        # => (batch..., C, d_out)\n",
    "        pre_ = pre_weight_acts[nm + \".hook_pre\"].detach()\n",
    "        w_ = component_weights[nm]\n",
    "        partial = einops.einsum(\n",
    "            pre_, w_, \"... d_in, C d_in d_out -> ... C d_out\"\n",
    "        )\n",
    "        component_acts[nm] = partial\n",
    "\n",
    "    for feature_idx in range(out_dim):\n",
    "        # sum up that scalar\n",
    "        # grads = autograd.grad(\n",
    "        #     model_out[..., feature_idx].sum(),\n",
    "        #     list(post_weight_acts.values()),\n",
    "        #     retain_graph=True,\n",
    "        # )\n",
    "        # grads = autograd.grad(\n",
    "        #     model_out[..., feature_idx].sum(),\n",
    "        #     list(post_weight_acts.values()),\n",
    "        #     retain_graph=True,\n",
    "        #     allow_unused=True  # Add this parameter\n",
    "        # )\n",
    "        # grads is tuple of same length as post_weight_acts\n",
    "        # feature_attrib = torch.zeros((*batch_prefix, C), device=model_out.device)\n",
    "        # for grad_val, nm_post in zip(grads, post_weight_acts.keys()):\n",
    "        #     nm_clean = nm_post.removesuffix(\".hook_post\")\n",
    "        #     feature_attrib += einops.einsum(\n",
    "        #         grad_val, component_acts[nm_clean],\n",
    "        #         \"... d_out, ... C d_out -> ... C\"\n",
    "        #     )\n",
    "\n",
    "        # In calc_grad_attributions, modify the autograd.grad call:\n",
    "        grads = autograd.grad(\n",
    "            model_out[..., feature_idx].sum(),\n",
    "            list(post_weight_acts.values()),\n",
    "            retain_graph=True,\n",
    "        )\n",
    "        feature_attrib = torch.zeros((*batch_prefix, C), device=model_out.device)\n",
    "        # Then handle potential None values in grads:\n",
    "        for grad_val, nm_post in zip(grads, post_weight_acts.keys()):\n",
    "            if grad_val is not None:  # Skip None gradients\n",
    "                nm_clean = nm_post.removesuffix(\".hook_post\")\n",
    "                feature_attrib += einops.einsum(\n",
    "                    grad_val, component_acts[nm_clean],\n",
    "                    \"... d_out, ... C d_out -> ... C\"\n",
    "                )\n",
    "        # square then accumulate\n",
    "        attribution_scores += feature_attrib**2\n",
    "\n",
    "    return attribution_scores\n",
    "\n",
    "\n",
    "def calculate_attributions(\n",
    "    model: SPDModel,\n",
    "    input_x: torch.Tensor,\n",
    "    out: torch.Tensor,\n",
    "    teacher_out: torch.Tensor,  # or target_out\n",
    "    pre_acts: dict[str, torch.Tensor],\n",
    "    post_acts: dict[str, torch.Tensor],\n",
    "    component_acts: dict[str, torch.Tensor],  # might not be used if gradient type\n",
    "    attribution_type: str = \"gradient\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    We unify \"ablation\", \"gradient\", or \"activation\" approach; for simplicity we do \"gradient\" or \"activation\".\n",
    "    \"\"\"\n",
    "    if attribution_type == \"activation\":\n",
    "        return calc_activation_attributions(component_acts)\n",
    "    elif attribution_type == \"gradient\":\n",
    "        # we call teacher_out or out as the \"model_out\"? It's up to you.\n",
    "        # Usually we do grad wrt target_out\n",
    "        # We'll do wrt teacher_out or spd_out. For TMS code, it's \"target_out\" or \"distil_from_target\".\n",
    "        # let's do teacher_out for distillation\n",
    "        # or do out if you want to see SPD's own gradient. We'll do teacher_out.\n",
    "        # If teacher_out is [B, 2], we do .sum() => need to check\n",
    "        return calc_grad_attributions(\n",
    "            model_out=teacher_out,\n",
    "            post_weight_acts={k: v for k, v in post_acts.items()},\n",
    "            pre_weight_acts={k: v for k, v in pre_acts.items()},\n",
    "            component_weights=collect_nested_module_attrs(model, \"component_weights\", include_attr_name=False),\n",
    "            C=model.C,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported attribution type {attribution_type}\")\n",
    "\n",
    "\n",
    "def calc_recon_mse(pred: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"MSE across batch, shape e.g. (batch, #classes) => scalar\"\"\"\n",
    "    return ((pred - ref) ** 2).mean(dim=-1).mean()\n",
    "\n",
    "\n",
    "###############################\n",
    "# The actual training script\n",
    "###############################\n",
    "\n",
    "from pydantic import BaseModel, PositiveInt, PositiveFloat, Field\n",
    "\n",
    "\n",
    "class WaterbirdSPDConfig(BaseModel):\n",
    "    # Basic\n",
    "    seed: int = 0\n",
    "    batch_size: PositiveInt = 32\n",
    "    steps: PositiveInt = 500\n",
    "    lr: float = 1e-3\n",
    "    print_freq: int = 50\n",
    "    save_freq: Optional[int] = None\n",
    "    out_dir: Optional[str] = None\n",
    "\n",
    "    # Distillation\n",
    "    distill_coeff: float = 1.0  # how strongly we do MSE with teacher\n",
    "    # If you want param match\n",
    "    param_match_coeff: float = 0.0\n",
    "\n",
    "    # For subcomponent #0 background detection\n",
    "    alpha_condition: float = 1.0\n",
    "\n",
    "    # SPD subcomponent config\n",
    "    C: PositiveInt = 40\n",
    "    m_fc1: PositiveInt = 16\n",
    "    m_fc2: PositiveInt = 16\n",
    "\n",
    "    # LR schedule\n",
    "    lr_schedule: str = \"constant\"  # or \"linear\", \"cosine\", \"exponential\"\n",
    "    lr_exponential_halflife: float | None = None\n",
    "    lr_warmup_pct: float = 0.0\n",
    "\n",
    "    unit_norm_matrices: bool = False\n",
    "    schatten_coeff: float | None = None\n",
    "    schatten_pnorm: float | None = None\n",
    "    # teacher ckpt\n",
    "    teacher_ckpt: str = \"waterbird_resnet18_best.pth\"\n",
    "\n",
    "    # topk config\n",
    "    topk: float | None = None\n",
    "    batch_topk: bool = True\n",
    "    topk_recon_coeff: float | None = None\n",
    "    distil_from_target: bool = True\n",
    "\n",
    "    # lp sparsity\n",
    "    lp_sparsity_coeff: float | None = None\n",
    "    pnorm: float | None = None\n",
    "\n",
    "    # attribution type\n",
    "    attribution_type: str = \"gradient\"  # or \"activation\"\n",
    "\n",
    "def set_As_and_Bs_to_unit_norm(spd_fc: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    In-place normalization so that each (A_c, B_c) has ||A_c||_F = 1 and/or ||B_c||_F = 1,\n",
    "    preventing unbounded scale growth in factorized weights.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # FC1\n",
    "        A1 = spd_fc.fc1.A  # shape [C, d_in, m_fc1]\n",
    "        B1 = spd_fc.fc1.B  # shape [C, m_fc1, hidden_dim]\n",
    "        # Normalize each subcomponent's A and B\n",
    "        for c in range(A1.shape[0]):\n",
    "            normA = A1[c].norm(p=2)\n",
    "            if normA > 1e-9:\n",
    "                A1[c] /= normA\n",
    "            normB = B1[c].norm(p=2)\n",
    "            if normB > 1e-9:\n",
    "                B1[c] /= normB\n",
    "\n",
    "        # FC2 (same logic; if you only want to do it for one layer, remove these lines)\n",
    "        A2 = spd_fc.fc2.A  # shape [C, hidden_dim, m_fc2]\n",
    "        B2 = spd_fc.fc2.B  # shape [C, m_fc2, num_classes]\n",
    "        for c in range(A2.shape[0]):\n",
    "            normA = A2[c].norm(p=2)\n",
    "            if normA > 1e-9:\n",
    "                A2[c] /= normA\n",
    "            normB = B2[c].norm(p=2)\n",
    "            if normB > 1e-9:\n",
    "                B2[c] /= normB\n",
    "\n",
    "\n",
    "def fix_normalized_adam_gradients(spd_fc: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Removes the gradient component corresponding to pure scale changes in each factor pair (A_c, B_c).\n",
    "    In rank factorization, scaling one factor by alpha and dividing the other by alpha is a no-op.\n",
    "    This prevents Adam from chasing that redundant direction.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # For each layer in SPD, do an orthogonal projection of grads\n",
    "        # that removes the direction that changes the norm scale of (A_c, B_c).\n",
    "\n",
    "        def _proj_out_scale_direction(A, B, gradA, gradB):\n",
    "            \"\"\"\n",
    "            If d d/d(alpha) [A*alpha, B/alpha] = 0 means no scale changes:\n",
    "            We'll find the derivative direction that changes scale\n",
    "            and zero it out from (gradA, gradB).\n",
    "            \"\"\"\n",
    "            # Flatten\n",
    "            A_vec = A.view(-1)\n",
    "            B_vec = B.view(-1)\n",
    "            gA_vec = gradA.view(-1)\n",
    "            gB_vec = gradB.view(-1)\n",
    "\n",
    "            # The \"scale\" direction is something like: d/d(alpha) [A*alpha, B/alpha],\n",
    "            # which at alpha=1 is (A, -B).\n",
    "            # So the direction vector is v = [A, -B].\n",
    "            # We compute the component of (gA, gB) in that direction and remove it.\n",
    "            v = torch.cat([A_vec, -B_vec], dim=0)\n",
    "            v_norm_sq = v.dot(v) + 1e-12\n",
    "\n",
    "            g = torch.cat([gA_vec, gB_vec], dim=0)\n",
    "            scale_coeff = g.dot(v) / v_norm_sq  # how much of g is in direction of v\n",
    "\n",
    "            # new_g = g - scale_coeff * v\n",
    "            new_g = g - scale_coeff * v\n",
    "\n",
    "            # put back\n",
    "            new_gA = new_g[:A_vec.shape[0]].view_as(gradA)\n",
    "            new_gB = new_g[A_vec.shape[0]:].view_as(gradB)\n",
    "\n",
    "            gradA.copy_(new_gA)\n",
    "            gradB.copy_(new_gB)\n",
    "\n",
    "        # FC1\n",
    "        A1 = spd_fc.fc1.A\n",
    "        B1 = spd_fc.fc1.B\n",
    "        if A1.requires_grad and B1.requires_grad:\n",
    "            for c in range(A1.shape[0]):\n",
    "                if A1.grad is not None and B1.grad is not None:\n",
    "                    gradA1 = A1.grad[c]\n",
    "                    gradB1 = B1.grad[c]\n",
    "                    _proj_out_scale_direction(A1[c], B1[c], gradA1, gradB1)\n",
    "\n",
    "        # FC2\n",
    "        A2 = spd_fc.fc2.A\n",
    "        B2 = spd_fc.fc2.B\n",
    "        if A2.requires_grad and B2.requires_grad:\n",
    "            for c in range(A2.shape[0]):\n",
    "                if A2.grad is not None and B2.grad is not None:\n",
    "                    gradA2 = A2.grad[c]\n",
    "                    gradB2 = B2.grad[c]\n",
    "                    _proj_out_scale_direction(A2[c], B2[c], gradA2, gradB2)\n",
    "\n",
    "\n",
    "def calc_schatten_loss(\n",
    "    As: dict[str, torch.Tensor],\n",
    "    Bs: dict[str, torch.Tensor],\n",
    "    mask: torch.Tensor,\n",
    "    p: float,\n",
    "    n_params: int,\n",
    "    device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate rank penalty: sum_{c} ||P_c||_p^p, where P_c = A_c * B_c^T,\n",
    "    implemented in a factorized form. \n",
    "    We multiply by a topk or attributions-based mask if desired (like in TMS).\n",
    "    \"\"\"\n",
    "    # We assume all layers in As and Bs have the same shape for dimension 0 (the \"C\" dimension).\n",
    "    # If you use topk_mask, shape should be (batch, C). We'll average over batch below.\n",
    "\n",
    "    assert As.keys() == Bs.keys(), \"As and Bs must have identical keys\"\n",
    "    batch_size = mask.shape[0]\n",
    "\n",
    "    schatten_penalty = torch.zeros((), device=device)  # scalar\n",
    "    for layer_name in As.keys():\n",
    "        # A: shape [C, d_in, m], B: shape [C, m, d_out]\n",
    "        # or possibly [batch, C, ...] if you handle multi-instance\n",
    "        A = As[layer_name]\n",
    "        B = Bs[layer_name]\n",
    "\n",
    "        # S_A = sum_{i,j} A^2 over d_in\n",
    "        # S_B = sum_{k,l} B^2 over d_out\n",
    "        # We'll do something like TMS: \n",
    "        #    S_A = einops.einsum(A, A, \"... C d_in m, ... C d_in m -> ... C m\")\n",
    "        #    S_B = einops.einsum(B, B, \"... C m d_out, ... C m d_out -> ... C m\")\n",
    "        # Then multiply S_AB = S_A * S_B, apply the topk mask, etc.\n",
    "\n",
    "        S_A = einops.einsum(A, A, \"... C d_in m, ... C d_in m -> ... C m\")\n",
    "        S_B = einops.einsum(B, B, \"... C m d_out, ... C m d_out -> ... C m\")\n",
    "        S_AB = S_A * S_B  # shape [batch..., C, m] or [C, m] if no batch dimension\n",
    "\n",
    "        # Now apply mask along the \"C\" dimension\n",
    "        # shape of mask is [batch, C]. We broadcast over the \"m\" dimension.\n",
    "        # We'll do an einsum:\n",
    "        # S_AB_topk = einops.einsum(S_AB, mask, \"... C m, batch C -> batch ... C m\")\n",
    "        # Then sum and do the p/2 exponent.\n",
    "        if S_AB.ndim == 2:\n",
    "            # no extra batch dimension on the parameter side => broadcast\n",
    "            # reshape so S_AB => (1, C, m) for broadcasting\n",
    "            S_AB = S_AB.unsqueeze(0)  # shape (1, C, m)\n",
    "\n",
    "        S_AB_topk = einops.einsum(S_AB, mask, \"b C m, b C -> b C m\")\n",
    "        # Now apply ( +1e-16 )^(0.5 * p)\n",
    "        schatten_penalty += ((S_AB_topk + 1e-16) ** (0.5 * p)).sum()\n",
    "\n",
    "    # normalizations\n",
    "    # 1) divide by number of parameters n_params\n",
    "    # 2) divide by batch_size\n",
    "    schatten_penalty = schatten_penalty / (n_params * batch_size)\n",
    "    return schatten_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = WaterbirdSPDConfig(\n",
    "    batch_size=32,\n",
    "    steps=500,\n",
    "    lr=1e-3,\n",
    "    print_freq=50,\n",
    "    save_freq=200,\n",
    "    out_dir=\"waterbird_spd_out\",\n",
    "    seed=0,\n",
    "    distill_coeff=1.0,\n",
    "    param_match_coeff=0.0,\n",
    "    alpha_condition=1.0,\n",
    "    C=40,\n",
    "    m_fc1=16,\n",
    "    m_fc2=16,\n",
    "    lp_sparsity_coeff=0.01,\n",
    "    pnorm=2.0,\n",
    "    topk=2.0,\n",
    "    batch_topk=True,\n",
    "    topk_recon_coeff=0.1,\n",
    "    teacher_ckpt=\"checkpoints/waterbird_resnet18_best.pth\",\n",
    "    attribution_type=\"gradient\",\n",
    "    lr_schedule=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_831433/3379874551.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(config.teacher_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(config.teacher_ckpt, map_location=\"cpu\")\n",
    "if \"model_state_dict\" in ckpt:\n",
    "    state_dict = ckpt[\"model_state_dict\"]\n",
    "else:\n",
    "    state_dict = ckpt \n",
    "\n",
    "\n",
    "# 1) load teacher\n",
    "teacher_model = WaterbirdResNet18(num_classes=2, hidden_dim=512)\n",
    "\n",
    "missing, unexpected = teacher_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_fc = SPDTwoLayerFC(\n",
    "    in_features=512,\n",
    "    hidden_dim=512,\n",
    "    num_classes=2,\n",
    "    C=config.C,\n",
    "    m_fc1=config.m_fc1,\n",
    "    m_fc2=config.m_fc2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_factor_params():\n",
    "    count = 0\n",
    "    for pname, pval in spd_fc.named_parameters():\n",
    "        if \"fc1\" in pname or \"fc2\" in pname:  # e.g. only final SPD layers\n",
    "            count += pval.numel()\n",
    "    return count\n",
    "n_params_spd = count_factor_params()\n",
    "\n",
    "# param names if we do param match\n",
    "param_names = [\"fc1\", \"fc2\"]\n",
    "\n",
    "waterbird_dataset = get_dataset(dataset=\"waterbirds\", download=False)\n",
    "\n",
    "all_indices = np.arange(len(waterbird_dataset))\n",
    "np.random.shuffle(all_indices)\n",
    "\n",
    "train_size = 2000 \n",
    "val_size = 1000 \n",
    "\n",
    "train_indices = all_indices[:train_size].tolist()\n",
    "\n",
    "val_indices = all_indices[train_size:train_size+val_size].tolist()\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),  # Adding data augmentation to reduce overfitting\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the training subset\n",
    "train_subset = WaterbirdsSubset(\n",
    "    waterbird_dataset, \n",
    "    indices=train_indices,\n",
    "    transform=train_transform\n",
    ")\n",
    "loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "trunk = teacher_model.features\n",
    "teacher_fc1 = teacher_model.fc1\n",
    "teacher_fc2 = teacher_model.fc2\n",
    "\n",
    "\n",
    "# 4) optimizer\n",
    "opt = optim.AdamW(spd_fc.parameters(), lr=config.lr, weight_decay=0.0)\n",
    "from spd.run_spd import get_lr_schedule_fn, get_lr_with_warmup\n",
    "lr_sched_fn = get_lr_schedule_fn(config.lr_schedule, config.lr_exponential_halflife)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 5) training loop\n",
    "steps_per_epoch = max(1, len(train_indices)//config.batch_size)\n",
    "data_iter = iter(loader)\n",
    "epoch = 0\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 \n",
    "step_lr = get_lr_with_warmup(\n",
    "    step=step,\n",
    "    steps=config.steps,\n",
    "    lr=config.lr,\n",
    "    lr_schedule_fn=lr_sched_fn,\n",
    "    lr_warmup_pct=config.lr_warmup_pct,\n",
    ")\n",
    "for g in opt.param_groups:\n",
    "    g[\"lr\"] = step_lr\n",
    "\n",
    "# fetch batch\n",
    "try:\n",
    "    batch_data = next(data_iter)\n",
    "except StopIteration:\n",
    "    epoch += 1\n",
    "    data_iter = iter(loader)\n",
    "    batch_data = next(data_iter)\n",
    "\n",
    "imgs, bird_label, meta = batch_data\n",
    "imgs = imgs.to(device)\n",
    "bird_label = bird_label.to(device)\n",
    "background_label = meta.float().to(device)  # 0 or 1\n",
    "\n",
    "opt.zero_grad(set_to_none=True)\n",
    "\n",
    "if config.unit_norm_matrices:\n",
    "    set_As_and_Bs_to_unit_norm(spd_fc)\n",
    "\n",
    "# Step 1: Extract features from trunk (no gradients needed for this part)\n",
    "with torch.no_grad():\n",
    "    feats = trunk(imgs)          # [B,512,1,1]\n",
    "    feats = feats.flatten(1)     # [B,512]\n",
    "\n",
    "# Step 2: Create tensor with gradients for teacher forward pass\n",
    "feats_with_grad = feats.detach().clone().requires_grad_(True)\n",
    "\n",
    "# Step 3: Teacher forward pass with manual activation caching\n",
    "teacher_cache = {}\n",
    "# Cache pre-activation for fc1\n",
    "teacher_cache[\"fc1.hook_pre\"] = feats_with_grad\n",
    "\n",
    "# Forward through fc1\n",
    "teacher_h_pre = teacher_fc1(feats_with_grad)\n",
    "teacher_cache[\"fc1.hook_post\"] = teacher_h_pre\n",
    "\n",
    "# Apply ReLU\n",
    "teacher_h = torch.relu(teacher_h_pre)\n",
    "teacher_cache[\"fc2.hook_pre\"] = teacher_h\n",
    "\n",
    "# Forward through fc2\n",
    "teacher_out = teacher_fc2(teacher_h)\n",
    "teacher_cache[\"fc2.hook_post\"] = teacher_out\n",
    "\n",
    "# Step 4: SPD forward pass with caching hooks\n",
    "spd_fc.reset_hooks()\n",
    "cache_dict, fwd, bwd = spd_fc.get_caching_hooks()\n",
    "with spd_fc.hooks(fwd_hooks=fwd, bwd_hooks=[], reset_hooks_end=True):\n",
    "    spd_h_pre = spd_fc.fc1(feats)  # Use the original feats (no grad needed)\n",
    "    spd_h = torch.relu(spd_h_pre)\n",
    "    spd_out = spd_fc.fc2(spd_h)\n",
    "\n",
    "# Step 5: Gather SPD activations from the cache\n",
    "pre_weight_acts = {}\n",
    "post_weight_acts = {}\n",
    "comp_acts = {}\n",
    "for k, v in cache_dict.items():\n",
    "    if k.endswith(\"hook_pre\"):\n",
    "        pre_weight_acts[k] = v\n",
    "    elif k.endswith(\"hook_post\"):\n",
    "        post_weight_acts[k] = v\n",
    "    elif k.endswith(\"hook_component_acts\"):\n",
    "        comp_acts[k] = v\n",
    "\n",
    "# Step 6: Split teacher activations into pre and post\n",
    "teacher_pre_acts = {k: v for k, v in teacher_cache.items() if k.endswith(\"hook_pre\")}\n",
    "teacher_post_acts = {k: v for k, v in teacher_cache.items() if k.endswith(\"hook_post\")}\n",
    "\n",
    "# Step 7: Calculate attributions\n",
    "attributions = calculate_attributions(\n",
    "    model=spd_fc,\n",
    "    input_x=feats,\n",
    "    out=spd_out,\n",
    "    teacher_out=teacher_out if config.distil_from_target else spd_out,\n",
    "    pre_acts=teacher_pre_acts,\n",
    "    post_acts=teacher_post_acts,\n",
    "    component_acts=comp_acts,\n",
    "    attribution_type=config.attribution_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_params = {}\n",
    "spd_params = {}\n",
    "for param_name in param_names:\n",
    "    target_params[param_name] = get_nested_module_attr(teacher_model, param_name + \".weight\")\n",
    "    spd_params[param_name] = get_nested_module_attr(spd_fc, param_name + \".weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc1': tensor([[-0.0014,  0.0033, -0.0019,  ..., -0.0006,  0.0006,  0.0003],\n",
       "         [ 0.0022, -0.0036, -0.0025,  ..., -0.0016,  0.0033,  0.0069],\n",
       "         [-0.0017, -0.0018, -0.0032,  ...,  0.0010, -0.0016,  0.0052],\n",
       "         ...,\n",
       "         [-0.0009, -0.0010, -0.0004,  ...,  0.0034,  0.0026, -0.0001],\n",
       "         [-0.0026, -0.0017,  0.0044,  ..., -0.0067, -0.0007,  0.0039],\n",
       "         [ 0.0027,  0.0042,  0.0021,  ...,  0.0015, -0.0005, -0.0066]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " 'fc2': tensor([[-0.0118, -0.0112],\n",
       "         [ 0.0582, -0.0097],\n",
       "         [ 0.1283,  0.0153],\n",
       "         ...,\n",
       "         [-0.0542, -0.0239],\n",
       "         [ 0.0123,  0.1034],\n",
       "         [ 0.0827,  0.0289]], device='cuda:0', grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_params['fc2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_params['fc2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
